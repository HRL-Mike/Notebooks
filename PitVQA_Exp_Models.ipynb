{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["s-AjfbXrfZBF"],"authorship_tag":"ABX9TyNdRp1fvRxEoJZhTELb3no4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### CLIP-GPT"],"metadata":{"id":"fwYhXij6fHja"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMr-nG-tdAtM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from transformers import CLIPProcessor, CLIPModel, CLIPConfig\n","from transformers import GPT2Model\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class CLIPGPTCLS(nn.Module):\n","    def __init__(self, num_class=18):\n","        super(CLIPGPTCLS, self).__init__()\n","\n","        # prepare CLIP encoders (visual and text)\n","        config = CLIPConfig.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        model = CLIPModel(config).to(device)\n","\n","        self.config = model.config  # 获取模型的配置信息, output_attentions, output_hidden_states\n","        self.text_model = model.text_model  # 获取文本编码模型\n","        self.vision_model = model.vision_model  # 获取图像编码模型\n","        self.visual_projection = nn.Linear(model.visual_projection.in_features, 768)  # 替换NN层的操作\n","        self.text_projection = nn.Linear(model.text_projection.in_features, 768)\n","        self.logit_scale = model.logit_scale  # 获取缩放因子，用于调整 logits\n","\n","        # GPT2 decoder\n","        self.VCA_decoder = GPT2Model.from_pretrained('gpt2')\n","\n","        # intermediate_layers\n","        self.intermediate_layer = nn.Linear(768, 512)\n","        self.LayerNorm = nn.BatchNorm1d(512)\n","        # self.LayerNorm = nn.LayerNorm(512)  # use this one if only one data point in each batch\n","        self.dropout = nn.Dropout(0.2)\n","\n","        # classifier\n","        self.classifier = nn.Linear(in_features=512, out_features=num_class)\n","\n","    def forward(\n","        self,\n","        input_ids=None,  # 非空, 文本id的数组\n","        pixel_values=None,  # 非空, 图像像素值的数组\n","        attention_mask=None,  # 非空, 注意力掩码\n","        position_ids=None,\n","        return_loss=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (output_hidden_states if output_hidden_states is not None\n","                                else self.config.output_hidden_states)\n","\n","        # use CLIP visual and text models to process data\n","        vision_outputs = self.vision_model(  # 使用视觉模型处理图像数据\n","            pixel_values=pixel_values,  # 图像的像素值数组\n","            output_attentions=output_attentions,  # True/False, 用来控制是否输出模型的注意力机制的细节\n","            output_hidden_states=output_hidden_states,  # True/False, 用来控制是否输出模型中间层的隐藏状态\n","            return_dict=return_dict,  # True/False, 被设置为True时, 模型的输出将被封装在一个字典中\n","        )\n","        text_outputs = self.text_model(  # 使用文本模型处理文本数据\n","            input_ids=input_ids,  # 文本ID的数组; 这个数组是模型输入文本的主要形式\n","            attention_mask=attention_mask,  # 0/1数组; 用于指示哪些部分的 input_ids 应该被模型考虑, 哪些部分是填充 (应该被模型忽略)\n","            position_ids=position_ids,  # 通常是整数数组, 用于表示输入中每个 token 的位置信息; 如果不提供, 模型通常会自动生成一个默认的位置编码\n","            output_attentions=output_attentions,  # True/False\n","            output_hidden_states=output_hidden_states,  # True/False\n","            return_dict=return_dict,  # True/False\n","        )\n","\n","        # get visual and text embeddings\n","        image_embeds = vision_outputs[0].to(device)\n","        image_embeds = self.visual_projection(image_embeds)  # 对图像嵌入进行投影处理\n","        text_embeds = text_outputs[0].to(device)\n","        text_embeds = self.text_projection(text_embeds)\n","\n","        batch_size = image_embeds.shape[0]  # 1\n","        visual_seq_len = image_embeds.shape[1]  # 50\n","\n","        # get text and visual attention mask\n","        text_attention_mask = attention_mask.to(device)\n","        visual_attention_mask = torch.ones((batch_size, visual_seq_len), dtype=torch.float).to(device)\n","\n","        # concatenate text and visual embeddings (text first)\n","        inputs_embeds = torch.cat((text_embeds, image_embeds), dim=1).to(device)  # 拼接2个512的embedding\n","        # concatenate text and visual attention mask (text first)\n","        inputs_attention_mask = torch.cat((text_attention_mask, visual_attention_mask), dim=1).to(device)\n","\n","        # decode\n","        decoder_output = self.VCA_decoder(inputs_embeds=inputs_embeds, attention_mask=inputs_attention_mask)\n","\n","        decoder_output = decoder_output.last_hidden_state.swapaxes(1, 2)\n","        decoder_output = F.adaptive_avg_pool1d(decoder_output, 1)\n","        decoder_output = decoder_output.swapaxes(1, 2).squeeze(1)\n","\n","        # intermediate layers\n","        out = self.intermediate_layer(decoder_output)\n","        out = self.LayerNorm(out)\n","        out = self.dropout(out)\n","\n","        # classifier\n","        out = self.classifier(out)\n","        return out"]},{"cell_type":"markdown","source":["### Surgical-GPT"],"metadata":{"id":"s-AjfbXrfZBF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models\n","\n","from transformers import VisualBertConfig, GPT2Config, GPT2Tokenizer\n","from transformers import VisualBertModel, GPT2Model\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class SurgicalGPTCLS(nn.Module):\n","    def __init__(self, num_class=18, vis_pos_emb=None):\n","        super(SurgicalGPTCLS, self).__init__()\n","\n","        # use default setting\n","        self.vis_pos_emb = vis_pos_emb\n","\n","        # image processing\n","        self.img_feature_extractor = models.resnet18(pretrained=True)  # ResNet18\n","        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n","\n","        # Visual embedding\n","        VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")  # VisualBert\n","        VB_config.visual_embedding_dim = 512\n","        visualbert = VisualBertModel(config=VB_config)\n","        self.visual_embedder = visualbert.embeddings.visual_projection\n","\n","        # Question embedding\n","        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","        question_embedder = GPT2Model.from_pretrained('gpt2')\n","        question_embedder.config.pad_token_id = tokenizer.eos_token\n","        self.question_embedder = question_embedder.wte  # word token embedding\n","\n","        # GPT2 decoder\n","        self.VCA_decoder = GPT2Model.from_pretrained('gpt2')\n","        self.VCA_decoder.config.pad_token_id = tokenizer.eos_token\n","\n","        # intermediate_layers\n","        self.intermediate_layer = nn.Linear(768, 512)  # (512+768)\n","        self.LayerNorm = nn.BatchNorm1d(512)\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # classifier\n","        self.classifier = nn.Linear(512, num_class)\n","\n","    def forward(self, input, img):\n","        # image encoder features\n","        img_feature = self.img_feature_extractor(img)\n","        img_feature = torch.unsqueeze(img_feature, dim=1)\n","\n","        # visual Embedding: id type 1, pos: zero / incremental\n","        visual_embeds = self.visual_embedder(img_feature)\n","        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n","        visual_attention_mask = visual_attention_mask.to(device)\n","\n","        if self.vis_pos_emb == 'zeroes':\n","            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n","            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n","        elif self.vis_pos_emb == 'pos':\n","            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n","            visual_position_id = torch.arange(0, visual_embeds.size()[1])\n","            visual_position_id = torch.unsqueeze(visual_position_id, 0)\n","            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n","            visual_position_id = visual_position_id.to(device)\n","\n","        # question embedding: id type 0, pose incremental\n","        input['input_ids'] = input['input_ids'].to(device)\n","        input['attention_mask'] = input['attention_mask'].to(device)\n","        question_embeds = self.question_embedder(input['input_ids'])\n","        question_attention_mask = input['attention_mask']\n","\n","        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n","            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n","            question_position_id = torch.arange(0, question_embeds.size()[1])\n","            question_position_id = torch.unsqueeze(question_position_id, 0)\n","            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n","            question_position_id = question_position_id.to(device)\n","\n","        # question first\n","        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)\n","        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n","\n","        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n","            token_type_ids = torch.cat((question_id_type, visual_id_type), dim=1)\n","            position_ids = torch.cat((question_position_id, visual_position_id), dim=1)\n","\n","        # VCA_GPT2 decoder\n","        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n","            decoder_output = self.VCA_decoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n","                                             position_ids=position_ids, token_type_ids=token_type_ids)\n","        else:\n","            decoder_output = self.VCA_decoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n","        decoder_output = decoder_output.last_hidden_state.swapaxes(1, 2)\n","        decoder_output = F.adaptive_avg_pool1d(decoder_output, 1)\n","        decoder_output = decoder_output.swapaxes(1, 2).squeeze(1)\n","\n","        # intermediate layers\n","        out = self.intermediate_layer(decoder_output)\n","        out = self.LayerNorm(out)\n","        out = self.dropout(out)\n","\n","        # classifier\n","        out = self.classifier(out)\n","        return out"],"metadata":{"id":"AhmKvZ8xjhOp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Surgical-LLaMA"],"metadata":{"id":"VaGwA_HYkypH"}},{"cell_type":"code","source":["from torch import nn\n","import torch.utils.data\n","from typing import Tuple\n","\n","from transformers import VisualBertConfig\n","from transformers import VisualBertModel, SwinModel\n","from transformers import LlamaForSequenceClassification\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class MLP(nn.Module):\n","    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n","        super(MLP, self).__init__()\n","        layers = []\n","        for i in range(len(sizes) - 1):\n","            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n","            if i < len(sizes) - 2:\n","                layers.append(act())\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.model(x)\n","\n","class SurgicalLlamaCLS(nn.Module):\n","    def __init__(self, num_class=59):\n","        super(SurgicalLlamaCLS, self).__init__()\n","        # image processing\n","        self.img_feature_extractor = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n","\n","        # Visual embedding\n","        VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n","        VB_config.visual_embedding_dim = 768\n","        visualbert = VisualBertModel(config=VB_config)\n","        self.visual_embedder = visualbert.embeddings.visual_projection\n","\n","        # projection layer\n","        llm_embedding_size = self.question_embedder.weight.shape[1]  # 4096\n","        image_feature_size = 768\n","        self.image_project = MLP((image_feature_size, llm_embedding_size // 2, llm_embedding_size))\n","\n","        # llama text encoder and decoder\n","        self.VCA_decoder = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\", num_labels=num_class)\n","        self.VCA_decoder.config.pad_token_id = self.VCA_decoder.config.eos_token_id\n","        self.question_embedder = self.VCA_decoder.model.embed_tokens\n","\n","    def forward(self, inputs, img):\n","\n","        # get image features\n","        img = img.to(device)\n","        img_feature = self.img_feature_extractor(pixel_values=img)\n","        # print('img_feature: ', img_feature[0].shape)  # torch.Size([1, 49, 768])\n","\n","        # visual embedding: input size 768, output size 4096\n","        visual_embeds = self.visual_embedder(img_feature[0])  # torch.Size([1, 49, 768])\n","        visual_embeds = self.image_project(visual_embeds)  # output: torch.Size([1, 49, 4096])\n","        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)  # [1, 49]\n","\n","        # question embedding\n","        inputs['input_ids'] = inputs['input_ids'].to(device)\n","        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n","\n","        question_embeds = self.question_embedder(inputs['input_ids'])  # 4096\n","        question_attention_mask = inputs['attention_mask']\n","\n","        # question first\n","        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)  # torch.Size([40, 74, 4096])\n","        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n","\n","        # VCA_GPT2 decoder\n","        decoder_output = self.VCA_decoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n","        logits = decoder_output.logits  # torch.Size([40, 18])\n","\n","        return logits\n","\n","\n","if __name__ == '__main__':\n","    from torchvision import transforms\n","    from PIL import Image\n","    from transformers import LlamaTokenizer\n","    from huggingface_hub import login\n","\n","    login(token='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')\n","\n","    preprocess = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    image = Image.open(\"./cat.jpg\")\n","    image = preprocess(image)\n","    img_tensor = image.unsqueeze(0)\n","\n","    # 加载llama分词器\n","    tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","    text = 'a photo of a cat'\n","\n","    # 处理文本\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n","    model = SurgicalLlamaCLS().to(device)\n","\n","    result = model(inputs, img_tensor)\n","    print(result)"],"metadata":{"id":"67BdQeGflAPh"},"execution_count":null,"outputs":[]}]}